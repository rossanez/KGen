Reusing existing datasets is of considerable significance to developers.
Reusing existing datasets is of considerable significance to researchers.
Dataset search engines help a user find relevant datasets for reuse.
Dataset search engines can present a snippet for each retrieved dataset to explain Dataset search engines relevance to the user's data needs.
This emerging problem of snippet generation for dataset search has not received much research attention.
To provide a basis for future research, we introduce a framework for quantitatively evaluating the quality of a dataset snippet.
The proposed metrics assess the extent to which a snippet matches the query intent.
The proposed metrics assess the extent to which a snippet covers the main content of each retrieved dataset to explain its relevance to the user's data needs.
To establish a baseline, we adapt four state-of-the-art methods from related fields to we problem.
To establish a baseline, we perform an empirical evaluation based on real-world datasets and queries.
we also conduct a user study to verify we findings.
The results suggest directions for future research.
The results demonstrate the effectiveness of we evaluation framework.